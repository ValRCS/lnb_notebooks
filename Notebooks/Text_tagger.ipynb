{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 776,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook is an extension to NLP-PIPE (https://nlp.ailab.lv) text tagger. It allows users to tag multiple files and download them in .json, .csv or .parquet format.\n",
    "# To tag text:\n",
    "#     1. Run first cell. Then choose tagging parameters and upload one or more .txt files you wish to tag.\n",
    "#     2. Run last cell and wait untill all files are tagged and download buttons appears.\n",
    "#     Then download tagged files in desired file format. All files will be placed in one zip file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d933322a2b049598b514eb3b560d148",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(FileUpload(value=(), accept='.txt', description='Upload', layout=Layout(margin='â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import logging\n",
    "import requests\n",
    "import base64\n",
    "import io\n",
    "import zipfile\n",
    "import codecs\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from math import ceil\n",
    "from sys import getsizeof\n",
    "from IPython.display import display\n",
    "from datetime import datetime\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from ipywidgets import HTML, FileUpload, ToggleButtons, Output, Select, Checkbox, HBox, VBox\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "API_URL = 'http://0.0.0.0:9500/api/nlp'\n",
    "param = [\"morpho\"]\n",
    "\n",
    "def iter_jsonl(fpath):\n",
    "    with open(fpath) as f:\n",
    "        for l in f:\n",
    "            yield json.loads(l)\n",
    "\n",
    "# Executes request\n",
    "def retry(f, n=3):\n",
    "    for i in range(n):\n",
    "        try:\n",
    "            return f()\n",
    "        except:\n",
    "            logging.exception('Try failed %s', i)\n",
    "    return None\n",
    "\n",
    "# Taggs text\n",
    "def process_text(text, steps=None):\n",
    "    global param\n",
    "    logger.debug('Process: %s', text)\n",
    "    t = text['text'][1]\n",
    "    fun = lambda: requests.post(API_URL, json={'steps': param, 'data': {'text': t}})\n",
    "    r = retry(fun)\n",
    "    if not r:\n",
    "        data = {'error': None}\n",
    "        logging.warning('Empty response %s', text)\n",
    "    elif r.status_code != 200:\n",
    "        logging.warning('Non 200 status code %s %s', r.status_code, text)\n",
    "        data = {'error': r.status_code}\n",
    "    else:\n",
    "        data = r.json()\n",
    "        if 'data' in data:\n",
    "            data = {'annotation': data['data']}\n",
    "        else:\n",
    "            logging.warning('error %s %s', data, text)\n",
    "\n",
    "    name = text['text'][0]\n",
    "    return (name, {**text, **data})\n",
    "\n",
    "# Processes files for tagging\n",
    "def process(texts, processes=1, verbose=False, log_each=100):\n",
    "    logging.info('Start processing with %d processes', processes)\n",
    "    ts = datetime.now()\n",
    "    with ThreadPool(processes) as p:\n",
    "        t = datetime.now()\n",
    "        i = 0\n",
    "        taged_texts = []\n",
    "        # for i, r in enumerate(p.imap(process_text, texts), start=1):\n",
    "        for i, r in tqdm(enumerate(p.imap_unordered(process_text, texts), start=1), total=len(texts), ascii=True, desc=\"Tagging files\"):\n",
    "            name = r[0]\n",
    "            r = r[1]\n",
    "            if i % log_each == 0:\n",
    "                logging.info('.. %d, took %s', i, datetime.now() - t)\n",
    "                t = datetime.now()\n",
    "            if verbose:\n",
    "                logger.info('%d %s', i, r)\n",
    "            taged_texts.append((name, json.dumps(r, ensure_ascii=False)))\n",
    "        logging.info('Finished processing %d docs, took %s', i, datetime.now() - ts)\n",
    "        return taged_texts\n",
    "\n",
    "def set_param(morpho_tokenizer, ner, parser):\n",
    "    ner = [\"ner\"] if ner else []\n",
    "    parser = [\"parser\"] if parser else []\n",
    "    \n",
    "    global param\n",
    "    if morpho_tokenizer == \"morpho\":\n",
    "        param = [\"morpho\"] + ner + parser \n",
    "    elif morpho_tokenizer == \"tokenizer\":\n",
    "        param = [\"tokenizer\"] + ner\n",
    "    else:\n",
    "        param = [param[0]] + ner + parser\n",
    "        \n",
    "# Returns widget for file unloading\n",
    "def upload_files():\n",
    "    upload = FileUpload(accept='.txt', multiple=True)\n",
    "    upload.layout.margin = \"20px 0px 30px 0px\"\n",
    "\n",
    "    uploaded_fnames = Output()\n",
    "    uploaded_fnames.layout.height = \"150px\"\n",
    "    uploaded_fnames.layout.overflow = \"auto auto\"\n",
    "\n",
    "    ner_btn = Checkbox(value=False, description=\"Named Entity Recognition\")\n",
    "    parser_btn = Checkbox(value=False, description=\"Dependency Parsing\")\n",
    "\n",
    "    ner_btn.layout.height = \"17px\"\n",
    "    ner_btn.layout.margin = \"0px\"\n",
    "    parser_btn.layout.margin = \"0px\"\n",
    "\n",
    "    checkboxes = VBox([ner_btn, parser_btn])\n",
    "    checkboxes.layout.height = \"50px\"\n",
    "\n",
    "    toggle = ToggleButtons(\n",
    "        options=[\"Morph. analizer\", \"Tokenizer\"],\n",
    "        description=\"Choose tagging parameters:\",\n",
    "        default=\"Morph. analizer\",\n",
    "        \n",
    "        display='flex',\n",
    "        flex_flow='column',\n",
    "        align_items='stretch', \n",
    "        style= {'description_width': 'initial'}\n",
    "    )\n",
    "    toggle.layout.height = \"80px\"\n",
    "    \n",
    "    def display_filenames(change):\n",
    "        if change[\"new\"]:\n",
    "            with uploaded_fnames:\n",
    "                for i in upload.value:\n",
    "                    print(i[\"name\"])\n",
    "    upload.observe(display_filenames, names=\"value\")\n",
    "\n",
    "    def on_select_change(change):\n",
    "        if change[\"new\"] == \"Tokenizer\":\n",
    "            parser_btn.value = False\n",
    "            set_param(\"tokenizer\", ner_btn.value, False)\n",
    "            checkboxes.children = [ner_btn]\n",
    "        elif change[\"new\"] == \"Morph. analizer\":\n",
    "            set_param(\"morpho\", ner_btn.value, parser_btn.value)\n",
    "            checkboxes.children = [ner_btn, parser_btn]\n",
    "        else:\n",
    "            set_param(False, ner_btn.value, parser_btn.value)\n",
    "    toggle.observe(on_select_change, names=\"value\")\n",
    "    ner_btn.observe(on_select_change, names=\"value\")\n",
    "    parser_btn.observe(on_select_change, names=\"value\")\n",
    "\n",
    "    options = HBox([toggle, checkboxes])\n",
    "    options.layout.align_items = \"flex-end\"\n",
    "    \n",
    "    return upload, options, uploaded_fnames\n",
    "\n",
    "# Splits text into <100Kb chunks. Each chunk ends with the end of the sentence.\n",
    "def split_text(text, text_size):      \n",
    "    splits = ceil(text_size/100_000)\n",
    "    interval = int(len(text)/splits)\n",
    "    parts = [text[i:i+interval] for i in range(0, len(text), interval)]\n",
    "\n",
    "    def spl_p(part):\n",
    "        excess, i = [], None\n",
    "        while True:\n",
    "            d = part.rfind(\".\", 0, i)\n",
    "            q = part.rfind(\"?\", 0, i)\n",
    "            e = part.rfind(\"!\", 0, i)\n",
    "            t = part.rfind(\"...\", 0, i)\n",
    "            m = max(d, q, e, t) + 1\n",
    "\n",
    "            excess.append(part[m:])\n",
    "            part = part[:m]\n",
    "\n",
    "            if getsizeof(part) < 100_000:\n",
    "                return part, \"\".join(excess[::-1])\n",
    "            i = -1\n",
    "\n",
    "    excess, new_parts = \"\", []\n",
    "    for p in parts:\n",
    "        part = excess + p        \n",
    "        part, excess = spl_p(part)\n",
    "                    \n",
    "        new_parts.append(part)\n",
    "\n",
    "    if getsizeof(excess) > 100_000:\n",
    "        part, excess = spl_p(excess)\n",
    "        new_parts.append(part)\n",
    "    new_parts.append(excess)\n",
    "    \n",
    "    new_parts = [x for x in new_parts if x != '']\n",
    "    return new_parts\n",
    "\n",
    "# Concatinates splitted files\n",
    "def concatenate_files(tagged_files):\n",
    "# tagged_files structure:\n",
    "# [('000_name.txt', '{\n",
    "#     \"text\": [\"000_name.txt\", \"Here is text from the file...\"],\n",
    "#     \"annotation\": {\n",
    "#         \"sentences\": [{\n",
    "#                 \"ner\": [{...}],                   # for ner\n",
    "#                 \"tokens\": [{\n",
    "#                     \"deprel\": \"obl\",              # for parser\n",
    "#                     \"features\": \"...\",            # for morpho\n",
    "#                     \"form\": \"Here\"                # for morpho and tokenizer\n",
    "#                     \"index\": 1,                   # for morpho\n",
    "#                     \"lemma\": \"Here\",              # for morpho\n",
    "#                     \"parent\": 2,                  # for parser\n",
    "#                     \"pos\": \"xf\",                  # for morpho\n",
    "#                     \"tag\": \"xf\"                   # for morpho\n",
    "#                     \"ufeats\": \"Foreign=Yes\",      # for parser\n",
    "#                     \"upos\": \"X\"                   # for parser\n",
    "#                  }, {...}]\n",
    "#         }, {# next sent ...}, {...}],\n",
    "#      \"text\": \"Here is text from the file...\" }\n",
    "# }'), ('001_name.txt', {...})]\n",
    "\n",
    "    text, sent, combined_files = [], [], []\n",
    "    name = tagged_files[0][0].split('_', 1)[1] # 'name.txt'\n",
    "    for file in tagged_files:\n",
    "        js = json.loads(file[1])\n",
    "        \n",
    "        # when old file name doesn't match new file name. Storing info in combined_files\n",
    "        if name != js['text'][0].split('_', 1)[1]:\n",
    "            c_text = \"\".join(text)\n",
    "            combined_files.append([name, {'text': c_text, 'annotation': {'sentences': sent, 'text': c_text}}])\n",
    "            text, sent = [], []\n",
    "\n",
    "        name = js['text'][0].split('_', 1)[1]\n",
    "        text.append(js['text'][1])\n",
    "        for s in js['annotation']['sentences']:\n",
    "            sent.append(s)\n",
    "    \n",
    "    c_text = \"\".join(text)\n",
    "    combined_files.append([name, {'text': c_text, 'annotation': {'sentences': sent, 'text': c_text}}])\n",
    "    return combined_files\n",
    "\n",
    "\n",
    "def procSentences(sentences, rows):\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for token in sentence.get('tokens'):\n",
    "            token.setdefault('sent_ndx', i)\n",
    "            if 'features' in token:\n",
    "                del token['features']\n",
    "            rows.append(token)\n",
    "    return rows\n",
    "\n",
    "\n",
    "def procChunk(chunk, row):\n",
    "    if 'annotation' not in chunk:\n",
    "        print(\"Missing data in this chunk!\")\n",
    "        return row\n",
    "    else:\n",
    "        return procSentences(chunk['annotation']['sentences'], row)\n",
    "\n",
    "\n",
    "def procChunks(chunks):\n",
    "    rows = []\n",
    "    for chunk in chunks:\n",
    "        rows = procChunk(chunk, rows)\n",
    "    return rows\n",
    "\n",
    "# Returns Dataframes with ners or tokens from JSON file \n",
    "def getDF(js):\n",
    "    rows = []\n",
    "    procChunk(js, rows)\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Prepares tagged files for download \n",
    "def download_files(tagged_files):\n",
    "    def get_zip_file(suffix):\n",
    "        zip_buffer = io.BytesIO()\n",
    "        with zipfile.ZipFile(zip_buffer, \"a\", zipfile.ZIP_DEFLATED, False) as zip_file:\n",
    "            if suffix == \"json\":\n",
    "                for name, data in tagged_files:\n",
    "                    name = name[:-4]+\".json\"\n",
    "                    data = json.dumps(data, ensure_ascii=False)\n",
    "                    zip_file.writestr(name, data)\n",
    "            else:\n",
    "                for name, data in tagged_files:\n",
    "                    df = getDF(data)\n",
    "                    if suffix == \"csv\":\n",
    "                        zip_file.writestr(f\"{name[:-4]}.csv\", df.to_csv())\n",
    "                    else:\n",
    "                        zip_file.writestr(f\"{name[:-4]}.parquet\", df.to_parquet())\n",
    "        \n",
    "        b64 = base64.b64encode(zip_buffer.getvalue())\n",
    "        return b64.decode()\n",
    "    html_buttons = '''<html>\n",
    "    <head>\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n",
    "    </head>\n",
    "    <body>\n",
    "    <div style=\"display: flex\">\n",
    "    \n",
    "    <div style=\"display: flex; flex-direction: column\">\n",
    "    <a download=\"tagged_{j_name[0]}_files_{j_name[1]}.zip\" href=\"data:text/csv;base64,{j_payload}\" download>\n",
    "    <button class=\"p-Widget jupyter-widgets jupyter-button widget-button mod-warning\">Download JSON</button>\n",
    "    </a>\n",
    "    </div>\n",
    "    \n",
    "    <div style=\"display: flex; flex-direction: column\">\n",
    "    <a download=\"tagged_{c_name[0]}_files_{c_name[1]}.zip\" href=\"data:text/csv;base64,{c_payload}\" download>\n",
    "    <button class=\"p-Widget jupyter-widgets jupyter-button widget-button mod-warning\">Download CSV</button>\n",
    "    </a>\n",
    "    </div>\n",
    "    \n",
    "    <div style=\"display: flex; flex-direction: column\">\n",
    "    <a download=\"tagged_{p_name[0]}_files_{p_name[1]}.zip\" href=\"data:text/csv;base64,{p_payload}\" download>\n",
    "    <button class=\"p-Widget jupyter-widgets jupyter-button widget-button mod-warning\">Download PARQUET</button>\n",
    "    </a>\n",
    "    </div>\n",
    "\n",
    "    </div>\n",
    "    </body>\n",
    "    </html>\n",
    "    '''\n",
    "    html_button = html_buttons.format(\n",
    "        j_payload = get_zip_file(\"json\"), j_name = ('json', \"_\".join(param)),\n",
    "        c_payload = get_zip_file(\"csv\"), c_name = ('csv', \"_\".join(param)),\n",
    "        p_payload = get_zip_file(\"parquet\"), p_name = ('paruqet', \"_\".join(param)),\n",
    "    )\n",
    "    display(HTML(html_button))\n",
    "    \n",
    "def tag_files():\n",
    "    logging.basicConfig(format='%(asctime)s : %(name)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "    \n",
    "    text_dict = []\n",
    "    for file in tqdm(upload.value, ascii=True, desc=\"Preparing files\"):\n",
    "        text = codecs.decode(file['content'], encoding=\"utf-8\") \n",
    "        name = file['name']\n",
    "        \n",
    "        text_size = getsizeof(text)\n",
    "        if text_size > 100_000:\n",
    "            s_texts = split_text(text, text_size)\n",
    "            text_dict += [{'text': (str(i).zfill(3)+\"_\"+name, t)} for i, t in enumerate(s_texts)]\n",
    "        else:\n",
    "            text_dict.append({'text': (\"_\"+name, text)})\n",
    "    \n",
    "    tagged_files = process(text_dict, processes=3, log_each=50)\n",
    "    tagged_files = concatenate_files(tagged_files)\n",
    "    download_files(tagged_files)\n",
    "\n",
    "upload, options, uploaded_fnames = upload_files()\n",
    "user_input = HBox([VBox([upload, options]), uploaded_fnames])\n",
    "display(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = tag_files()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
